<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Uncertainty-Driven Knowledge Distillation for
    Language Model Compression</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="../../assets/img/favicon.png" rel="icon">
  <link href="../../assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Roboto:300,300i,400,400i,500,500i,700,700i&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="../../assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="../../assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="../../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="../../assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="../../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="../../assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="../../assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="../../assets/css/style.css" rel="stylesheet">
  <link href="../../assets/css/image_slider.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Moderna - v4.10.1
  * Template URL: https://bootstrapmade.com/free-bootstrap-template-corporate-moderna/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center ">
    <div class="container d-flex justify-content-between align-items-center">

      <div class="logo">
        <h1 class="text-light"><a href="../../index.html"><span>VRL Lab</span></a></h1>
        <!-- Uncomment below if you prefer to use an image logo -->
        <!-- <a href="index.html"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>-->
      </div>

      <nav id="navbar" class="navbar">
        <ul>
            <li><a class="" href="../../index.html">Home</a></li>
            <li><a href="../../research.html">Research</a></li>
            <li><a class="active" href="../../publication.html">Publication</a></li>
            <li><a href="../../team.html">Team</a></li>
            <li><a href="../../demo.html">Demo</a></li>
            <li><a href="../../contact.html">Join Us</a></li>
          </li>

        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <main id="main">

    <!-- ======= Blog Section ======= -->
    <section class="breadcrumbs">
      <div class="container">
        <div class="d-flex justify-content-between align-items-center">
          <ol>
            <li><a href="../../index.html">Home</a></li>
            <li><a href="../../research.html">Research</a></li>
            <li>Uncertainty-Driven Knowledge Distillation for
              Language Model Compression </li>
          </ol>
        </div>

      </div>
    </section><!-- End Blog Section -->

    <!-- ======= Blog Single Section ======= -->
    <section id="blog" class="blog">
      <div class="container" data-aos="fade-up">
        
        <h2>Project</h2>
            <article class="entry entry-single">

                <h1 class="entry-title">
                    <a href="./index.html">Uncertainty-Driven Knowledge Distillation for
                      Language Model Compression</a>
                </h1>

                <h5><li class="d-flex align-items-center"><i class="bi bi-person"></i> Xiaotong Lu, Han Huang, Weisheng Dong, Xin Li and Guangming Shi</li></h5>

                <!-- <div class="a">
                    <div class="b" id="b">
                        <div><img src="./HR/img_014.png" alt="Before"></div>
                    </div>
                    <img src="./LR/img_014_LR.png" alt="After">
                    <input type="range" id="range" value="50">
                </div> -->

                <img src="./img/network.png" class="img-fluid" alt="">
                <p>
                  Fig. 1. Workflow of our model compression scheme. (a) N-to-1 compression compresses the stacked N consecutive layers of the transformer module into one
                  transformer module. (b) Uncertainty-driven distillation architecture (when N = 2) includes copying and freezing the parameters of the odd-numbered multi-head
                  attention (MHA) layer as well as imposing MSE constraints on the feed-forward network (FFN) of the even-numbered layer. We propose an FFN loss function and
                  uncertainty estimate module (UEM) to improve knowledge distillation in pretrained language model compression.

 
                </p>

                <div class="entry-content">
                    <h3>Abstract</h3>
                    <p>
                    &emsp;Despite the remarkable performance on various Nat-
                    ural Language Processing (NLP) tasks, the parametric complexity
                    of pretrained language models has remained a major obstacle due
                    to limited computational resources in many practical applications.
                    Techniques such as knowledge distillation, network pruning, and
                    quantization have been developed for language model compression.
                    However, it has remained challenging to achieve an optimal tradeoff
                    between model size and inference accuracy. To address this issue,
                    we propose a novel and efficient uncertainty-driven knowledge
                    distillation compression method for transformer-based pretrained
                    language models. Specifically, we design a method of parame-
                    ter retention and feedforward network parameter distillation to
                    compress N-stacked transformer modules into one module in the
                    fine-tuning stage. A key innovation of our approach is to add the un-
                    certainty estimation module (UEM) into the student network such
                    that it can guide the student network’s feature reconstruction in
                    the latent space (similar to the teacher’s). Across multiple datasets
                    in the natural language inference tasks of GLUE, we have achieved
                    more than 95% accuracy of the original BERT, while only using
                    about 50% of the parameters.

 

                    </p>

                    <h3>Paper & Code & Demo</h3>
                    <ul>
                        <p>
                            <i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Papers/Journal/Uncertainty-Driven_Knowledge_Distillation_for_Language_Model_Compression.pdf?pdf=button">Paper</a>
                        </p>
                        <!-- <p>
                            <i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Code_release/Mod-NAS.zip">Code</a>
                        </p> -->
                        <!-- <p>
                            <i class="bi bi-check"></i> <a href="https://github.com/Fangzhenxuan/UncertaintySR">Demo</a>
                        </p> -->
                    </ul>
                    
                    <h3>Experimental Results</h3>
                    <ul>
                        <p>
                          Fig. 1. Distribution of sigma values.
                        </p>
                        <img src="./img/fig_1.png" class="img-fluid" alt="">
                        <p>
                          Fig. 2. Parameter analysis
                        </p>
                        <img src="./img/fig_2.png" class="img-fluid" alt="">
                        <p>
                          Table 1. PERFORMANCE ON DIFFERENT DATASETS
                        </p>
                        <img src="./img/table_1.png " class="img-fluid" alt="">
                        <p>
                          Table 2. EXPERIMENTAL R ESULTS OF DIFFERENT C OMPRESS R ATE N
                        </p>
                        <img src="./img/table_2.png " class="img-fluid" alt="">

                    </ul>

                    <!-- <h3>Result Visualization</h3>
                    <ul>
                        <img src="./img/vision_result_1.png" class="img-fluid" alt="">
                        
                        <p>
                          
                        </p>


                    </ul> -->

                    <h3>Citation</h3>
                    <p>

                      @article{huang2023uncertainty,<br>
                        &emsp;title={Uncertainty-Driven Knowledge Distillation for Language Model Compression},<br>
                        &emsp;author={Huang, Tianyu and Dong, Weisheng and Wu, Fangfang and Li, Xin and Shi, Guangming},<br>
                        &emsp;journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},<br>
                        &emsp;year={2023},<br>
                        &emsp;publisher={IEEE}<br>
                      }
                      
                    </p>
                    
                    <h3>Concat</h3>
                    <p> 
                        <strong>Tianyu Huang </strong>, Email: 19171213910@stu.xidian.edu.cn; <br>
                        <strong>Weisheng Dong</strong>, Email: wsdong@mail.xidian.edu.cn <br>
                        <strong>Xin Li</strong>, Email: xin.li@mail.wvu.edu <br>
                        <strong>Fangfang Wu</strong>, Email: wufangfang@xidian.edu.cn  <br>
                        <strong>Guangming Shi</strong>, Email: gmshi@xidian.edu.cn <br>
                    </p>
                  
                </div>
            </article><!-- End blog entry -->

            

      </div>
    </section><!-- End Blog Single Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer" data-aos="fade-up" data-aos-easing="ease-in-out" data-aos-duration="500">

    <div class="footer-top">
      <div class="container">
        <div class="row">
          <div class="col-lg-3 col-md-6 footer-contact">
            <h4>Contact Us</h4>
            <p>
              School of Artificial Intelligence <br>
              Xidian University <br>
              8 Taibai South Road, Xi 'an, China<br>
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="copyright">
        &copy;<span>VRL Lab@XDU, 2023</span></strong>
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="../../assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="../../assets/vendor/aos/aos.js"></script>
  <script src="../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../../assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="../../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="../../assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="../../assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="../../assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="../../assets/js/main.js"></script>
  <script src="../../assets/js/image_slider.js"></script>

</body>

</html>