<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">

    <title>VRL-lab - Index</title>
    <meta content="" name="description">
    <meta content="" name="keywords">

    <!-- Favicons -->
    <link href="assets/img/favicon.png" rel="icon">
    <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Roboto:300,300i,400,400i,500,500i,700,700i&display=swap" rel="stylesheet">

    <!-- Vendor CSS Files -->
    <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
    <link href="assets/vendor/aos/aos.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
    <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
    <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

    <!-- Template Main CSS File -->
    <link href="assets/css/style.css" rel="stylesheet">
    <!--  <link href="assets/css/theme.css" rel = "stylesheet">-->

    <!-- =======================================================
    * Template Name: Moderna - v4.10.1
    * Template URL: https://bootstrapmade.com/free-bootstrap-template-corporate-moderna/
    * Author: BootstrapMade.com
    * License: https://bootstrapmade.com/license/
    ======================================================== -->
</head>

<body>

<!-- ======= Header ======= -->
<header id="header" class="fixed-top d-flex align-items-center header-transparent">
    <div class="container d-flex justify-content-between align-items-center">

        <div class="logo">
            <h1 class="text-light"><a href="index.html"><span>VRL-Lab</span></a></h1>
            <!-- Uncomment below if you prefer to use an image logo -->
            <!-- <a href="index.html"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>-->
        </div>

        <nav id="navbar" class="navbar">
            <ul>
                <li><a class="" href="index_zh.html">中文版</a></li>
                <li><a href="research.html">Research</a></li>
                <li><a href="publication.html">Publication</a></li>
                <li><a href="team.html">People</a></li>
                <li><a href="demo.html">Demo</a></li>
                <li><a href="contact.html">Join Us</a></li>
            </ul>
            <i class="bi bi-list mobile-nav-toggle"></i>
        </nav><!-- .navbar -->

    </div>
</header><!-- End Header -->

<!-- ======= Hero Section ======= -->
<section id="hero" class="d-flex justify-cntent-center align-items-center">
    <div id="heroCarousel" class="container carousel carousel-fade" data-bs-ride="carousel" data-bs-interval="5000">

        <!-- Slide 1 -->
        <div class="carousel-item active">
            <div class="carousel-container">
                <h2 class="animate__animated animate__fadeInDown"> <span>Visual Representation and Learning  Lab</span></h2>
                <p class="animate__animated animate__fadeInUp">Dedicated to research on image denoising, image superresolution, and image recognition in computer vision.</p>
                <a href="research.html" class="btn-get-started animate__animated animate__fadeInUp">Read More</a>
            </div>
        </div>

<!--        &lt;!&ndash; Slide 2 &ndash;&gt;-->
<!--        <div class="carousel-item">-->
<!--            <div class="carousel-container">-->
<!--                <h2 class="animate__animated animate__fadeInDown">董老师课题组</h2>-->
<!--                <p class="animate__animated animate__fadeInUp">在index.html 第101行修改.</p>-->
<!--                <a href="" class="btn-get-started animate__animated animate__fadeInUp">Read More</a>-->
<!--            </div>-->
<!--        </div>-->

        <a class="carousel-control-prev" href="#heroCarousel" role="button" data-bs-slide="prev">
            <span class="carousel-control-prev-icon bx bx-chevron-left" aria-hidden="true"></span>
        </a>

        <a class="carousel-control-next" href="#heroCarousel" role="button" data-bs-slide="next">
            <span class="carousel-control-next-icon bx bx-chevron-right" aria-hidden="true"></span>
        </a>

    </div>
</section><!-- End Hero -->

<main id="main">

    <!-- about Section -->
    <section class="why-us section-bg" data-aos="fade-up" date-aos-delay="200">
        <div class="container-fluid px-0" data-aos="fade-up">
            <div class="row">
                <div class="col-md-6 py-6 py-lg-8 bg-blue-light overflow-hidden">
                    <div class="row h-100 align-items-center">
                        <div class="col px-5 px-lg-8 px-xl-10">
                            <!--                <h3 class="my-0 fs-1 fw-medium text-primary text-uppercase text-center text-lg-left">About</h3>-->
                            <h2 class="my-3 fw-medium text-secondary text-uppercase text-center text-lg-left ">About</h2>
                            <h2 class="mb-4 fw-medium text-secondary text-center text-lg-left">VRL Lab@XDU</h2>
                            <p class="mb-0 text-justify">VRL Lab@XDU was formed on the 1 January 2018, with a research focus on computer vision and deep learning. It is now a group with four faculty members and more than 30 members including research fellows, research assistants, and PhD students. <br /><br />
                            </p>
                            <p class="mb-0 text-justify"> Members in VRL Lab@XDU conduct research primarily in low-level vision, image and video understanding, denoising and super-resolutioning, 3D scene understanding and reconstruction(Nerf). Have a look at the overview of <a href="./research.html">our research</a>. All publications are listed <a href="./publication.html">here</a>. <br /><br />
                            </p>
                            <p class="mb-0 text-justify"> We are always looking for motivated PhD students, postdocs, research assistants who have the same interests like us. Please check out the <a href="./contact.html">Join us</a> page.
                            </p>
                        </div>
                    </div>
                </div>
                <div class="col-md-6 py-8">
                    <img src="./assets/img/test.jpg" class="img-fluid" alt="">
                    <!--            <div class="bg-container" style="background-image: url(./assets/img/favicon.png);"></div>-->
                </div>
            </div>
        </div>
    </section>
    <!-- End about Section -->

    <!-- ======= Services Section ======= -->
    <section class="services">
        <div class="container">
            <div class="section-title">
                <h2>Research Direction</h2>
                <p>Here are our research interests</p>
            </div>
                
            <br>
            <div class="row">
                <div class="col-md-6 col-lg-3 d-flex align-items-stretch" data-aos="fade-up">
                    <div class="icon-box icon-box-pink">
                        <div class="icon"><i class="bx bxl-dribbble"></i></div>
                        <h4 class="title"> learning, low rank/sparse representation</a></h4>
<!--                        <p class="description">Voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident</p>-->
                    </div>
                </div>

                <div class="col-md-6 col-lg-3 d-flex align-items-stretch" data-aos="fade-up" data-aos-delay="100">
                    <div class="icon-box icon-box-cyan">
                        <div class="icon"><i class="bx bx-file"></i></div>
                        <h4 class="title">Image/video representation and processing</a></h4>
<!--                        <p class="description">Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur</p>-->
                    </div>
                </div>

                <div class="col-md-6 col-lg-3 d-flex align-items-stretch" data-aos="fade-up" data-aos-delay="200">
                    <div class="icon-box icon-box-green">
                        <div class="icon"><i class="bx bx-tachometer"></i></div>
                        <h4 class="title">Computer Vision and Pattern Recognition</a></h4>
<!--                        <p class="description">Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum</p>-->
                    </div>
                </div>

                <div class="col-md-6 col-lg-3 d-flex align-items-stretch" data-aos="fade-up" data-aos-delay="200">
                    <div class="icon-box icon-box-blue">
                        <div class="icon"><i class="bx bx-world"></i></div>
                        <h4 class="title">Remote sensing image processing and understanding</a></h4>
<!--                        <p class="description">At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque</p>-->
                    </div>
                </div>

            </div>

        </div>
    </section><!-- End Services Section -->


    <!-- ======= Features Section ======= -->
    <section class="features">
        <div class="container">

            <div class="section-title">
                <h2>Newest Papers</h2>
                <p>Here are the papers written by some students and teacher of us. The newest paper is on the top.</p>
                <p>Some of the papers are published with codes. You can download them if we upload the files to our cloud.</p>
            </div>

            <!--        新增论文部分-->
            <!--        格式：-->
            <!--        <div class="md-7 pt-4" data-aos="fade-up">-->
            <!--          <h3>文章标题</h3>-->
            <!--          <h5>文章作者</h5>-->
            <!--          <h6>发表于什么期刊/会议，哪一年</h6>-->

            <!--          <img src="网络结构图，路径是assets/papers/期刊or会议_年份_作者的姓_network.png（jpg）" class="img-fluid" alt="">-->
            <!--          <ul>-->
            <!--            <h5>Abstract</h5>-->
            <!--            文章摘要-->
            <!--            <li><i class="bi bi-check"></i> <a href="论文链接">Paper</a></li>-->
            <!--            <li><i class="bi bi-check"></i> <a href="代码链接">Code</a></li>-->
            <!--            -->
            <!--          </ul>-->
            <!--        </div>-->



            <!-- TPAMI_2023_Lu -->
            <div class="row" data-aos="fade-up">
                <div class="col-md-7 pt-4" data-aos="fade-up">
                    <h3>Adaptive Search-and-Training for Robust and Efficient Network Pruning</h3>
                    <h5>X. Lu, W. Dong*, X. Li, J. Wu, L Li, and G. Shi</h5>
                    <h6>TPAMI 2023</h6>
                    <ul class="faq-list">
                        <br>
                        <li>
                            <div data-bs-toggle="collapse" class="collapsed question" href="#abstract_Lu_TPAMI_2023"> <h5>Abstract</h5> <i class="bi bi-chevron-down icon-show"></i><i class="bi bi-chevron-up icon-close"></i></div>
                            <div id="abstract_Lu_TPAMI_2023" class="collapse" data-bs-parent=".faq-list">
                                <p>
                                    Both network pruning and neural architecture search (NAS) can be interpreted as techniques to automate the design and optimization of 
                                    artificial neural networks. In this paper, we challenge the conventional wisdom of training before pruning by proposing a joint 
                                    search-and-training approach to learn a compact network directly from scratch. Using pruning as a search strategy, we advocate three new 
                                    insights for network engineering: 1) to formulate adaptive search as a cold start strategy to find a compact subnetwork on the coarse 
                                    scale; and 2) to automatically learn the threshold for network pruning; 3) to offer flexibility to choose between efficiency and robustness. 
                                    More specifically, we propose an adaptive search algorithm in the cold start by exploiting the randomness and flexibility of filter pruning. 
                                    The weights associated with the network filters will be updated by ThreshNet, a flexible coarse-to-fine pruning method inspired by 
                                    reinforcement learning. In addition, we introduce a robust pruning strategy leveraging the technique of knowledge distillation through a 
                                    teacher-student network. Extensive experiments on ResNet and VGGNet have shown that our proposed method can achieve a better balance in terms 
                                    of efficiency and accuracy and notable advantages over current state-of-the-art pruning methods in several popular datasets, including 
                                    CIFAR10, CIFAR100, and ImageNet.
                                </p>
                            </div>                               
                        </li>
                    </ul>
                </div>
                <div class="col-md-5 pt-4" data-aos="fade-up" style="display: table;">
                    <img src="project\lxt_tpami2023\img\network.png" class="img-fluid" alt="" style="display: inline-block;">
                    <li><i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Papers/Journal/PAMI2023_final.pdf">Paper</a></li>
                    <li><i class="bi bi-check"></i> <a href="https://github.com/Lxtccc/Adaptive-">Code</a></li>
                    <li><i class="bi bi-check"></i> Demo</li>
                </div>
                <br>
                <br>
            </div>

            <!-- TIP_2023_Ning -->
            <div class="row" data-aos="fade-up">
                <div class="col-md-7 pt-4" data-aos="fade-up">
                    <h3>Searching Efficient Model-Guided Deep Network for Image Denoising</h3>
                    <h5>Q. Ning, W. Dong*, X. Li, and J. Wu</h5>
                    <h6>TIP 2023</h6>
                    <ul class="faq-list">
                        <br>
                        <li>
                            <div data-bs-toggle="collapse" class="collapsed question" href="#abstract_Ning_TIP_2023"> <h5>Abstract</h5> <i class="bi bi-chevron-down icon-show"></i><i class="bi bi-chevron-up icon-close"></i></div>
                            <div id="abstract_Ning_TIP_2023" class="collapse">
                                <p>
                                    Unlike the success of neural architecture search (NAS) in high-level vision tasks, it remains challenging to find computationally efficient and 
                                    memory-efficient solutions to low-level vision problems such as image restoration throughNAS. One of the fundamental barriers to differential NAS based 
                                    image restoration is the optimization gap between the super-network and the sub-architectures, causing instability during the searching process. In this 
                                    paper, we present a novel approach to fill this gap in image denoising application by connecting model-guided design (MoD) with NAS (MoDNAS). Specifically, 
                                    we propose to construct a new search space under a model-guided framework and develop more stable and efficient differential search strategies. MoD-NAS 
                                    employs a highly reusable width search strategy and a densely connected search block to automatically select the operations of each layer as well as 
                                    network width and depth via gradient descent. During the search process, the proposed MoD-NAS remains stable because of the smoother search space designed 
                                    under the model-guided framework. Experimental results on several popular datasets show that our MoD-NAS method has achieved at least comparable even 
                                    better PSNR performance than current state-of-the-art methods with fewer parameters, fewer flops, and less testing time.
                                </p>
                            </div>                               
                        </li>       
                    </ul>
                </div>
                <div class="col-md-5 pt-4" data-aos="fade-up" style="display: table;">
                    <img src="project\nq_tip_2023\img\network.png" class="img-fluid" alt="">
                    <li><i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Papers/Journal/Searching_Efficient_Model-Guided_Deep_Network_for_Image_Denoising_TIP-2023.pdf">Paper</a></li>
                    <li><i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Code_release/Mod-NAS.zip">Code</a></li>
                    <li><i class="bi bi-check"></i> Demo</li>
                </div>
                <br>
                <br>
            </div>           

            <!--  CVPR_2023_Fang -->
            <div class="row" data-aos="fade-up">
                <div class="col-md-7 pt-4" data-aos="fade-up">
                    <h3>Self-supervised Non-uniform Kernel Estimation with Flow-based Motion Prior for Blind Image Deblurring</h3>
                    <h5> Z. Fang, F. Wu*, W. Dong, X. Li, J. Wu, and G. Shi</h5>
                    <h6>CVPR 2023</h6>
                    <ul class="faq-list">
                        <br>
                        <li>
                            <div data-bs-toggle="collapse" class="collapsed question" href="#abstract_Fang_CVPR_2023"> <h5>Abstract</h5> <i class="bi bi-chevron-down icon-show"></i><i class="bi bi-chevron-up icon-close"></i></div>
                            <div id="abstract_Fang_CVPR_2023" class="collapse">
                                <p>
                                    Many deep learning-based solutions to blind image deblurring estimate the blur representation and reconstruct the target image from 
                                    its blurry observation. However, these methods suffer from severe performance degradation in real-world scenarios because they ignore 
                                    important prior information about motion blur (e.g., real-world motion blur is diverse and spatially varying). Some methods have attempted 
                                    to explicitly estimate non-uniform blur kernels by CNNs, but accurate estimation is still challenging due to the lack of ground truth 
                                    about spatially varying blur kernels in real-world images. To address these issues, we propose to represent the field of motion blur 
                                    kernels in a latent space by normalizing flows, and design CNNs to predict the latent codes instead of motion kernels. To further 
                                    improve the accuracy and robustness of non-uniform kernel estimation, we introduce uncertainty learning into the process of estimating 
                                    latent codes and propose a multi-scale kernel attention module to better integrate image features with estimated kernels. Extensive 
                                    experimental results, especially on real-world blur datasets, demonstrate that our method achieves state-of-the-art results in terms 
                                    of both subjective and objective quality as well as excellent generalization performance for non-uniform image deblurring.
                                </p>
                            </div>                               
                        </li>                      
                    </ul>
                </div>
                <div class="col-md-5 pt-4" data-aos="fade-up" style="display: table;">
                    <img src="project\fzx_cvpr2023\img\network.png" class="img-fluid" alt="">
                    <li><i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Projects/UFPNet.files/Self-supervised%20Non-uniform%20Kernel%20Estimation%20with%20Flow-based%20Motion%20Prior.pdf">Paper</a></li>
                    <li><i class="bi bi-check"></i> <a href="https://github.com/Fangzhenxuan/UFPDeblur">Code</a></li>
                    <li><i class="bi bi-check"></i> Demo</li>
                </div>
                <br>
                <br>
            </div>

                
            <!-- CVPR_2023_Yang -->
            <div class="row" data-aos="fade-up">
                <div class="col-md-7 pt-4" data-aos="fade-up">
                    <h3>Vector Quantization with Self-attention for Quality-independent Representation Learning</h3>
                    <h5>Z. Yang, W. Dong*, X. Li, Y. Sun, M. Huang and G. Shi</h5>
                    <h6>CVPR 2023</h6>
                    <ul class="faq-list">
                    <div>
                        <br>
                        <li>
                            <div data-bs-toggle="collapse" class="collapsed question" href="#abstract_Yang_CVPR_2023"> <h5>Abstract</h5> <i class="bi bi-chevron-down icon-show"></i><i class="bi bi-chevron-up icon-close"></i></div>
                            <div id="abstract_Yang_CVPR_2023" class="collapse">
                                <p>
                                    Recently, the robustness of deep neural networks has drawn extensive attention due to the potential distribution shift between 
                                    training and testing data (e.g., deep models trained on high-quality images are sensitive to corruption during testing). Many researchers 
                                    attempt to make the model learn invariant representations from multiple corrupted data through data augmentation or image-pair-based 
                                    feature distillation to improve the robustness. Inspired by sparse representation in image restoration, we opt to address this issue by 
                                    learning image-quality-independent feature representation in a simple plug-and-play manner, that is, to introduce discrete vector 
                                    quantization (VQ) to remove redundancy in recognition models. Specifically, we first add a codebook module to the network to quantize deep 
                                    features. Then we concatenate them and design a self-attention module to enhance the representation. During training, we enforce the 
                                    quantization of features from clean and corrupted images in the same discrete embedding space so that an invariant {\em quality-independent} 
                                    feature representation can be learned to improve the recognition robustness of low-quality images. Qualitative and quantitative experimental results show 
                                    that our method achieved this goal effectively, leading to a new state-of-the-art result of 43.1 $\%$ mCE on ImageNet-C with ResNet50 as the backbone.
                                </p>
                            </div>                               
                        </li>  
                    </div>                        
                    </ul>
                </div>
                <div class="col-md-5 pt-4" data-aos="fade-up" style="display: table;">
                    <img src="project\yz_cvpr2023\img\network.jpg" class="img-fluid" alt="">
                    <li><i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Papers/Conference/VQSA.pdf">Paper</a></li>
                    <li><i class="bi bi-check"></i> <a href="https://github.com/yangzhou321/VQSA">Code</a></li>
                    <li><i class="bi bi-check"></i> Demo</li>
                </div>
                <br>
                <br>
            </div>
    
                 
    </section><!-- End Features Section -->

</main><!-- End #main -->

<!-- ======= Footer ======= -->
<footer id="footer" data-aos="fade-up" data-aos-easing="ease-in-out" data-aos-duration="500">

    <div class="footer-top">
        <div class="container">
            <div class="row">
                <div class="col-lg-3 col-md-6 footer-contact">
                    <h4>Contact Us</h4>
                    <p>
                        School of Artificial Intelligence <br>
                        Xidian University <br>
                        8 Taibai South Road, Xi 'an, China<br>
                        wsdong@mail.xidian.edu.cn
                    </p>
                </div>
            </div>
        </div>
    </div>

    <div class="container">
			<div class="copyright">
					&copy;VRL Lab@XDU, 2023  Powered by <a href="https://jerryw1120.github.io/">Yichen Wang</a>
			</div>
    </div>
</footer><!-- End Footer -->


<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

</body>

</html>