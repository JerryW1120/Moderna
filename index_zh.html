<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">

    <title>VRL lab - Index</title>
    <meta content="" name="description">
    <meta content="" name="keywords">

    <!-- Favicons -->
    <link href="assets/img/favicon.png" rel="icon">
    <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Roboto:300,300i,400,400i,500,500i,700,700i&display=swap" rel="stylesheet">

    <!-- Vendor CSS Files -->
    <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
    <link href="assets/vendor/aos/aos.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
    <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
    <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

    <!-- Template Main CSS File -->
    <link href="assets/css/style.css" rel="stylesheet">
    <!--  <link href="assets/css/theme.css" rel = "stylesheet">-->

    <!-- =======================================================
    * Template Name: Moderna - v4.10.1
    * Template URL: https://bootstrapmade.com/free-bootstrap-template-corporate-moderna/
    * Author: BootstrapMade.com
    * License: https://bootstrapmade.com/license/
    ======================================================== -->
</head>

<body>

<!-- ======= Header ======= -->
<header id="header" class="fixed-top d-flex align-items-center header-transparent">
    <div class="container d-flex justify-content-between align-items-center">

        <div class="logo">
            <h1 class="text-light"><a href="index.html"><span>VRL lab</span></a></h1>
            <!-- Uncomment below if you prefer to use an image logo -->
            <!-- <a href="index.html"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>-->
        </div>

        <nav id="navbar" class="navbar">
            <ul>
                <li><a class="" href="index.html">Eng</a></li>
                <li><a href="research.html">最新研究</a></li>
                <li><a href="publication.html">文章发表</a></li>
                <li><a href="team.html">实验团队</a></li>
                <li><a href="demo.html">成果演示</a></li>
                <li><a href="contact.html">联系我们</a></li>
            </ul>
            <i class="bi bi-list mobile-nav-toggle"></i>
        </nav><!-- .navbar -->

    </div>
</header><!-- End Header -->

<!-- ======= Hero Section ======= -->
<section id="hero" class="d-flex justify-cntent-center align-items-center">
    <div id="heroCarousel" class="container carousel carousel-fade" data-bs-ride="carousel" data-bs-interval="5000">

        <!-- Slide 1 -->
        <div class="carousel-item active">
            <div class="carousel-container">
                <h2 class="animate__animated animate__fadeInDown">视觉表征与学习实验室</h2>
                <p class="animate__animated animate__fadeInUp">致力于计算机视觉的图像去噪、图像超分辨和图像识别等各项研究。</p>
                <a href="research.html" class="btn-get-started animate__animated animate__fadeInUp">查看更多</a>

            </div>
        </div>

<!--        &lt;!&ndash; Slide 2 &ndash;&gt;-->
<!--        <div class="carousel-item">-->
<!--            <div class="carousel-container">-->
<!--                <h2 class="animate__animated animate__fadeInDown"> <span>Visual Representation and Learning  Lab</span></h2>-->
<!--                <p class="animate__animated animate__fadeInUp">Dedicated to research on image denoising, image superresolution, and image recognition in computer vision.</p>-->
<!--                <a href="" class="btn-get-started animate__animated animate__fadeInUp">Read More</a>-->
<!--            </div>-->
<!--        </div>-->

        <a class="carousel-control-prev" href="#heroCarousel" role="button" data-bs-slide="prev">
            <span class="carousel-control-prev-icon bx bx-chevron-left" aria-hidden="true"></span>
        </a>

        <a class="carousel-control-next" href="#heroCarousel" role="button" data-bs-slide="next">
            <span class="carousel-control-next-icon bx bx-chevron-right" aria-hidden="true"></span>
        </a>

    </div>
</section><!-- End Hero -->

<main id="main">

    <!-- about Section -->
    <section class="why-us section-bg" data-aos="fade-up" date-aos-delay="200">
        <div class="container-fluid px-0" data-aos="fade-up">
            <div class="row">
                <div class="col-md-6 py-6 py-lg-8 bg-blue-light overflow-hidden">
                    <div class="row h-100 align-items-center">
                        <div class="col px-5 px-lg-8 px-xl-10">
                            <!--                <h3 class="my-0 fs-1 fw-medium text-primary text-uppercase text-center text-lg-left">About</h3>-->
                            <h2 class="my-3 fw-medium text-secondary text-uppercase text-center text-lg-left ">关于我们</h2>
                            <h2 class="mb-4 fw-medium text-secondary text-center text-lg-left">视觉表征与学习实验室</h2>
                            <p class="mb-0 text-justify">视觉表征与学习实验室始建于2018年1月1日，主要从事深度学习中的计算机视觉研究。现有教职人员2名，博士研究生和硕士研究生共30多名。 <br /><br />
<!--                            </p>-->
                            <p class="mb-0 text-justify"> VRL实验室的主要研究方向包括底层视觉、图像和视频理解、图像和视频去噪、图像和视频超分辨、3D场景理解和重构和图像识别和检测等。点击查看各项<a href="./research.html">工作</a>的内容介绍和<a href="publication.html">论文</a>。 <br /><br />
                            </p>
                            <p class="mb-0 text-justify"> 欢迎任何对我们研究内容和方向感兴趣的学生和学者通过下列邮箱与我们联系：wsdong@mail.xidian.edu.cn</a>。
                            </p>
                        </div>
                    </div>
                </div>
                <div class="col-md-6 py-8">
                    <img src="./assets/img/test.jpg" class="img-fluid" alt="">
                    <!--            <div class="bg-container" style="background-image: url(./assets/img/favicon.png);"></div>-->
                </div>
            </div>
        </div>
    </section>
    <!-- End about Section -->

    <!-- ======= Services Section ======= -->
    <section class="services">
        <div class="container">
            <div class="section-title">
                <h2>研究方向</h2>
                <p>以下是我们实验室主要的研究方向</p>
            </div>
            <br>
            <div class="row">
                <div class="col-md-6 col-lg-3 d-flex align-items-stretch" data-aos="fade-up">
                    <div class="icon-box icon-box-pink">
                        <div class="icon"><i class="bx bxl-dribbble"></i></div>
                        <h4 class="title">深度学习和稀疏表示 </a></h4>
<!--                        <p class="description">Deep Learning and Sparse Representation</p>-->
                    </div>
                </div>

                <div class="col-md-6 col-lg-3 d-flex align-items-stretch" data-aos="fade-up" data-aos-delay="100">
                    <div class="icon-box icon-box-cyan">
                        <div class="icon"><i class="bx bx-file"></i></div>
                        <h4 class="title">图像和视频的表示与处理</a></h4>
<!--                        <p class="description">Image and Video Representation and Processing</p>-->
                    </div>
                </div>

                <div class="col-md-6 col-lg-3 d-flex align-items-stretch" data-aos="fade-up" data-aos-delay="200">
                    <div class="icon-box icon-box-green">
                        <div class="icon"><i class="bx bx-tachometer"></i></div>
                        <h4 class="title">计算机视觉和模式识别</a></h4>
<!--                        <p class="description">Computer Vision and Pattern Recognition</p>-->
                    </div>
                </div>

                <div class="col-md-6 col-lg-3 d-flex align-items-stretch" data-aos="fade-up" data-aos-delay="200">
                    <div class="icon-box icon-box-blue">
                        <div class="icon"><i class="bx bx-world"></i></div>
                        <h4 class="title">遥感图像的理解与学习</a></h4>
<!--                        <p class="description"> Remote Sensing Images Understanding and Learning</p>-->
                    </div>
                </div>

            </div>

        </div>
    </section><!-- End Services Section -->


    <!-- ======= Features Section ======= -->
    <section class="features">
        <div class="container">

            <div class="section-title">
                <h2>最新工作</h2>
                <p>下面是我们实验室的一些最新研究进展</p>
                <p>部分工作已开源代码，欢迎您阅读，和我们讨论、交流！</p>
            </div>

            <!--        新增论文部分-->
            <!--        格式：-->
            <!--        <div class="md-7 pt-4" data-aos="fade-up">-->
            <!--          <h3>文章标题</h3>-->
            <!--          <h5>文章作者</h5>-->
            <!--          <h6>发表于什么期刊/会议，哪一年</h6>-->

            <!--          <img src="网络结构图，路径是assets/papers/期刊or会议_年份_作者的姓_network.png（jpg）" class="img-fluid" alt="">-->
            <!--          <ul>-->
            <!--            <h4>Abstract</h4>-->
            <!--            文章摘要-->
            <!--            <li><i class="bi bi-check"></i> <a href="论文链接">Paper</a></li>-->
            <!--            <li><i class="bi bi-check"></i> <a href="代码链接">Code</a></li>-->
            <!--            -->
            <!--            如果有的话会放一个streamlit的框，用于演示-->
            <!--          </ul>-->
            <!--        </div>-->



            <!-- ECCV_2022_Fang -->
            <div class="row" data-aos="fade-up">
                <div class="col-md-7 pt-4" data-aos="fade-up">
                    <h3>Uncertainty learning in kernel estimation for multi-stage blind image super-resolution</h3>
                    <h5>Z. Fang, W. Dong*, X. Li, J. Wu, L. Li, and G. Shi</h5>
                    <h6>ECCV 2022</h6>
                    <ul class="faq-list">
                        <br>
                        <li>
                            <div data-bs-toggle="collapse" class="collapsed question" href="#abstract_Fang_ECCV_2022"> <h5>Abstract</h5> <i class="bi bi-chevron-down icon-show"></i><i class="bi bi-chevron-up icon-close"></i></div>
                            <div id="abstract_Fang_ECCV_2022" class="collapse" data-bs-parent=".faq-list">
                                <p>
                                    Conventional wisdom in blind super-resolution (SR) first estimates the unknown
                                    degradation from the low-resolution image and then
                                    exploits the degradation information for image reconstruction. Such sequential
                                    approaches suffer from two fundamental weaknesses - i.e., the
                                    lack of robustness (the performance drops when the estimated degradation
                                    is inaccurate) and the lack of transparency (network architectures
                                    are heuristic without incorporating domain knowledge). To address these
                                    issues, we propose a joint Maximum a Posteriori (MAP) approach for
                                    estimating the unknown kernel and high-resolution image simultaneously.
                                    Our method first introduces uncertainty learning in the latent space when
                                    estimating the blur kernel, aiming at improving the robustness to the
                                    estimation error. Then we propose a novel SR network by unfolding the
                                    joint MAP estimator with a learned Laplacian Scale Mixture (LSM) prior
                                    and the estimated kernel. We have also developed a novel approach of
                                    estimating both the scale prior coefficient and the local means of the
                                    LSM model through a deep convolutional neural network (DCNN). All
                                    parameters of the MAP estimation algorithm and the DCNN parameters
                                    are jointly optimized through end-to-end training. Extensive experiments
                                    on both synthetic and real-world images show that our method achieves
                                    state-of-the-art performance for the task of blind image SR.
                                </p>
                            </div>                               
                        </li>
                    </ul>
                </div>
                <div class="col-md-5 pt-4" data-aos="fade-up" style="display: table;">
                    <img src="project/fzx_eccv2022/img/network1.png" class="img-fluid" alt="" style="display: inline-block;">
                    <li><i class="bi bi-check"></i> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136780141.pdf">Paper</a></li>
                    <li><i class="bi bi-check"></i> <a href="https://github.com/Fangzhenxuan/UncertaintySR">Code</a></li>
                    <li><i class="bi bi-check"></i> <a href="https://demo_fzx.vrl-lab.org">Demo</a></li>
                </div>
                <br>
                <br>
            </div>

            <!-- ECCV_2022_Yang -->
            <div class="row" data-aos="fade-up">
                <div class="col-md-7 pt-4" data-aos="fade-up">
                    <h3>Self-feature distillation with uncertainty modeling for degraded image recognition</h3>
                    <h5>Z. Yang, W. Dong*, X. Li, J. Wu, L. Li, and G. Shi</h5>
                    <h6>ECCV 2022</h6>
                    <ul class="faq-list">
                        <br>
                        <li>
                            <div data-bs-toggle="collapse" class="collapsed question" href="#abstract_Yang_ECCV_2022"> <h5>Abstract</h5> <i class="bi bi-chevron-down icon-show"></i><i class="bi bi-chevron-up icon-close"></i></div>
                            <div id="abstract_Yang_ECCV_2022" class="collapse">
                                <p>
                                    Despite the remarkable performance on high-quality (HQ)
                                    data, the accuracy of deep image recognition models degrades rapidly
                                    in the presence of low-quality (LQ) images. Both feature de-drifting and
                                    quality agnostic models have been developed to make the features extracted
                                    from degraded images closer to those of HQ images. In these
                                    methods, the l2-norm is usually used as a constraint. It treats each pixel
                                    in the feature equally and may result in relatively poor reconstruction
                                    performance in some difficult regions. To address this issue, we propose a
                                    novel self-feature distillation method with uncertainty modeling for better
                                    producing HQ-like features from low-quality observations in this paper.
                                    Specifically, in a standard recognition model, we use the HQ features to
                                    distill the corresponding degraded ones and conduct uncertainty modeling
                                    according to the diversity of degradation sources, increasing the
                                    weights of feature regions that is difficult to recover in the distillation
                                    loss. Experiments demonstrate that our method can extract HQ-like fea
                                    tures better even when the inputs are degraded images, which makes the
                                    model more robust than other approaches.
                                </p>
                            </div>                               
                        </li>       
                    </ul>
                </div>
                <div class="col-md-5 pt-4" data-aos="fade-up" style="display: table;">
                    <img src="project/yz_eccv2022/img/network.png" class="img-fluid" alt="">
                    <li><i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Papers/Conference/Self-Feature%20Distillation_ECCV2022.pdf">Paper</a></li>
                    <li><i class="bi bi-check"></i> Code</li>
                    <li><i class="bi bi-check"></i> Demo</li>
                </div>
                <br>
                <br>
            </div>           

            <!--  IJCAI_2022_Ning -->
            <div class="row" data-aos="fade-up">
                <div class="col-md-7 pt-4" data-aos="fade-up">
                    <h3>Learning degradation Uncertainty for unsupervised real-world image super-resolution</h3>
                    <h5> Q. Ning, J. Tang, F. Wu, W. Dong*, X. Li, and G. Shi</h5>
                    <h6>IJCAI 2022</h6>
                    <ul class="faq-list">
                        <br>
                        <li>
                            <div data-bs-toggle="collapse" class="collapsed question" href="#abstract_Ning_IJCAI_2022"> <h5>Abstract</h5> <i class="bi bi-chevron-down icon-show"></i><i class="bi bi-chevron-up icon-close"></i></div>
                            <div id="abstract_Ning_IJCAI_2022" class="collapse">
                                <p>
                                    Acquiring degraded images with paired highresolution (HR) images is often 
                                    challenging, impeding the advance of image super-resolution in
                                    real-world applications. By generating realistic
                                    low-resolution (LR) images with degradation similar to that in real-world scenarios, 
                                    simulated paired LR-HR data can be constructed for supervised
                                    training. However, most of the existing work ignores the degradation 
                                    uncertainty of the generated realistic LR images, since only one LR image
                                    has been generated given an HR image. To address
                                    this weakness, we propose learning the degradation
                                    uncertainty of generated LR images and sampling
                                    multiple LR images from the learned LR image
                                    (mean) and degradation uncertainty (variance) and
                                    construct LR-HR pairs to train the super-resolution
                                    (SR) networks. Specifically, uncertainty can be
                                    learned by minimizing the proposed loss based on
                                    Kullback-Leibler (KL) divergence. Furthermore,
                                    the uncertainty in the feature domain is exploited
                                    by a novel perceptual loss; and we propose to calculate the adversarial loss 
                                    from the gradient information in the SR stage for stable training performance 
                                    and better visual quality. Experimental results on popular real-world datasets 
                                    show that our proposed method has performed better than other unsupervised approaches.
                                </p>
                            </div>                               
                        </li>                      
                    </ul>
                </div>
                <div class="col-md-5 pt-4" data-aos="fade-up" style="display: table;">
                    <img src="project/nq_ijcai2022/img/network.png" class="img-fluid" alt="">
                    <li><i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Papers/Conference/IJCAI-2022.pdf">Paper</a></li>
                    <li><i class="bi bi-check"></i> Code</li>
                    <li><i class="bi bi-check"></i> Demo</li>
                </div>
                <br>
                <br>
            </div>

                
            <!-- CVPR_2021_Huang -->
            <div class="row" data-aos="fade-up">
                <div class="col-md-7 pt-4" data-aos="fade-up">
                    <h3>Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging</h3>
                    <h5>T. Huang, W. Dong*, X. Yuan*, J. Wu, and G. Shi</h5>
                    <h6>CVPR 2021</h6>
                    <ul class="faq-list">
                    <div>
                        <br>
                        <li>
                            <div data-bs-toggle="collapse" class="collapsed question" href="#abstract_Huang_CVPR_2021"> <h5>Abstract</h5> <i class="bi bi-chevron-down icon-show"></i><i class="bi bi-chevron-up icon-close"></i></div>
                            <div id="abstract_Huang_CVPR_2021" class="collapse">
                                <p>
                                    In coded aperture snapshot spectral imaging (CASSI)
                                    system, the real-world hyperspectral image (HSI) can be reconstructed 
                                    from the captured compressive image in a snapshot. Model-based HSI reconstruction methods employed
                                    hand-crafted priors to solve the reconstruction problem, but
                                    most of which achieved limited success due to the poor 
                                    representation capability of these hand-crafted priors. Deep
                                    learning based methods learning the mappings between the
                                    compressive images and the HSIs directly achieved much
                                    better results. Yet, it is nontrivial to design a powerful deep
                                    network heuristically for achieving satisfied results. In this
                                    paper, we propose a novel HSI reconstruction method based
                                    on the Maximum a Posterior (MAP) estimation framework
                                    using learned Gaussian Scale Mixture (GSM) prior. Different 
                                    from existing GSM models using hand-crafted scale
                                    priors (e.g., the Jeffrey’s prior), we propose to learn the
                                    scale prior through a deep convolutional neural network
                                    (DCNN). Furthermore, we also propose to estimate the local means 
                                    of the GSM models by the DCNN. All the parameters of the MAP estimation algorithm and the DCNN
                                    parameters are jointly optimized through end-to-end training. 
                                    Extensive experimental results on both synthetic and
                                    real datasets demonstrate that the proposed method outperforms existing state-of-the-art methods. 
                                </p>
                            </div>                               
                        </li>  
                    </div>                        
                    </ul>
                </div>
                <div class="col-md-5 pt-4" data-aos="fade-up" style="display: table;">
                    <img src="project/ht_cvpr2021/img/network.png" class="img-fluid" alt="">
                    <li><i class="bi bi-check"></i> <a href="https://see.xidian.edu.cn/faculty/wsdong/Papers/Conference/Huang_Deep_Gaussian_Scale_Mixture_Prior_for_Spectral_Compressive_Imaging_CVPR_2021_paper.pdf">Paper</a></li>
                    <li><i class="bi bi-check"></i> Code</li>
                    <li><i class="bi bi-check"></i> Demo</li>
                </div>
                <br>
                <br>
            </div>
    
                 
    </section><!-- End Features Section -->

</main><!-- End #main -->

<!-- ======= Footer ======= -->
<footer id="footer" data-aos="fade-up" data-aos-easing="ease-in-out" data-aos-duration="500">

    <div class="footer-top">
        <div class="container">
            <div class="row">
                <div class="col-lg-3 col-md-6 footer-contact">
                    <h4>联系我们</h4>
                    <p>
                        西安电子科技大学 人工智能学院<br>
                        陕西省西安市雁塔区电子城街道太白南路2号<br>
                        wsdong@mail.xidian.edu.cn
                    </p>
                </div>
            </div>
        </div>
    </div>

    <div class="container">
    	<div class="copyright">
        &copy;VRL Lab@XDU, 2023  Powered by <a href="https://jerryw1120.github.io/">Yichen Wang</a>
    	</div>
    </div>
</footer><!-- End Footer -->


<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

<!-- Vendor JS Files -->
<script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
<script src="assets/vendor/aos/aos.js"></script>
<script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
<script src="assets/vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="assets/js/main.js"></script>

</body>

</html>